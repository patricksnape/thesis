%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kernel PCA}\label{sec:kpca}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Given a set of, $K$, $F$-dimensional data vectors stacked in a
matrix $\mathbf{X} = [\mathbf{x}_1, \ldots, \mathbf{x}_K] \in \R^F$, 
we assume the existence of a positive semi-definite kernel function 
$k(\circ, \circ) : \R^F \times \R^F \rightarrow \R$. Given that $k(\circ, \circ)$ 
is positive semi-definite we can use it to define the inner product in an 
arbitrary dimensional Hilbert space, $\hilbert$, which we will call the feature 
space. There then exists an implicit mapping, $\Phi$, from the input 
space $\R^F$ to the feature space, $\hilbert$:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
    \begin{aligned}\label{eq:implicit-map}
        \Phi : \R^F \rightarrow \hilbert, \; \; \mathbf{x} \rightarrow \Phi(\mathbf{x})
    \end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Due to the often implicit nature of the mapping $\Phi$, we need only the kernel 
function since 
$\langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}_j) \rangle  = k (\mathbf{x}_i, \mathbf{x}_j)$, 
the so-called kernel trick. Now, component analysis within the feature space 
is equivalent to
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
    \begin{aligned}\label{eq:feature-space-pca}
        \underset{\mathbf{U}_\Phi}{\arg\max} \; \mathbf{U}_\Phi^T \bar{\mathbf{X}}_\Phi \bar{\mathbf{X}}_\Phi^T \mathbf{U}_\Phi \qquad \text{s.t.} \; \mathbf{U}_\Phi^T \mathbf{U}_\Phi = \mathbf{I}
    \end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $\mathbf{U}_\Phi = [\mathbf{U}_\Phi^1, ..., \mathbf{U}_\Phi^P] \in \hilbert$,
$\mathbf{m}_\Phi = \frac{1}{K} \sum \limits_{i=1}^K \Phi(\mathbf{x_i})$ and
$\bar{\mathbf{X}}_\Phi = [\Phi(\mathbf{x_i}) - \mathbf{m}_\Phi, ..., \Phi(\mathbf{x_K}) - \mathbf{m}_\Phi]$.

By noting that 
$\mathbf{\bar{X}}_\Phi \mathbf{\bar{X}}_\Phi^T = (\mathbf{X}_\Phi \mathbf{M}) (\mathbf{X}_\Phi \mathbf{M})^T$, 
where $\mathbf{M} = \mathbf{I} - \frac{1}{K} \mathbf{1} \mathbf{1}^T$ and
$\mathbf{1}$ represents a vector of ones, we can find
$\mathbf{U}_\Phi$ by performing eigenanalysis on
$\bar{\mathbf{X}}_\Phi^T \bar{\mathbf{X}}_\Phi$. Therefore,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
    \begin{aligned}\label{eq:x-bar-corr}
        \mathbf{\bar{X}}_\Phi^T \mathbf{\bar{X}}_\Phi = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^T \mathbf{U}_{\Phi} &= \mathbf{\bar{X}}_\Phi^T \mathbf{V} \boldsymbol{\Lambda}^{-\frac{1}{2}}
    \end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Though $\mathbf{U}_\Phi$ can be defined, it cannot be calculated explicitly. 
However, we can compute the KPCA-transformed feature vector
 $\mathbf{\tilde{y}} = [\mathbf{y}_1, ..., \mathbf{y}_K]$ by:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
    \begin{aligned}\label{eq:projections}
        \mathbf{\tilde{y}} = \mathbf{U}_\Phi^T \Phi(\mathbf{y}) &= \boldsymbol{\Lambda}^{-\frac{1}{2}} \mathbf{V}^T \mathbf{\bar{X}}_\Phi^T \Phi(\mathbf{y}) \\
        &= \boldsymbol{\Lambda}^{-\frac{1}{2}} \mathbf{V}^T \mathbf{M} \mathbf{X}_\Phi^T \Phi(\mathbf{y})
    \end{aligned}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We can, therefore, define the projection in terms of the kernel function
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}\label{eq:kernel-vector}
        \mathbf{X}_\Phi^T \Phi(\mathbf{y}) = \left[ k(\mathbf{y}_1, \mathbf{x}_1), \ldots, k(\mathbf{y}_K, \mathbf{x}_K) \right]^T
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Reconstruction of a vector can be performed by
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}\label{eq:vector-reconstruction}
        \bb{\tilde{X}} = {\Phi}^{-1} \left( \mathbf{U}_{\Phi} {\mathbf{U}_{\Phi}}^T (\Phi(\mathbf{x}) - \mathbf{m}_{\Phi}) + \mathbf{m}_{\Phi} \right)
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Unfortunately, since ${\Phi}^{-1}$ rarely exists or is extremely expensive to 
compute, performing reconstruction using \cref{eq:vector-reconstruction} is not 
generally feasible. In these cases, reconstruction can be performed by means of 
pre-images~\cite{kwok2004pre}. However, in the case of the kernels we propose
for normals, ${\Phi}^{-1}$ does exist and explicit mapping between the space of
normals and kernel space is performed. Finally, we should note here that in the 
general KPCA framework it is not necessary to subtract the mean. In this case, 
KPCA can be seen in the perspective of metric multi-dimensional 
scaling~\cite{williams2002connection}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
