%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Single Image: Statistical Models of Surface Normals}\label{ch:singl_imag}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\minitoc{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The most difficult scenario for shape recovery is recovery from a single
image. As discussed in \cref{sec:bg_sfs}, Shape-from-shading (SfS) provides a
principled way to recover shape from single images of human faces. In this case,
the shape recovered is parametrised in the form of surface normals,
which determine how light interacts with the surface of the face.
However, given that the general
shape-from-shading case is ill-defined, it would be beneficial to augment
shape-from-shading with prior knowledge that constrains the recovered shape to
lie in the space of plausible facial shapes. The work of
\citet{smith2006recovering,smith2008facial}
provides a statistical framework for shape recovery based around regularising
the output of a SfS algorithm with a parametric linear model of normals.
Parametric linear models are readily constructed from facial data and have
been successfully used for both 2D~\cite{cootes2001active,turk1991eigenfaces}
and 3D~\cite{enciso1999synthesis,atick1996statistical} analysis of faces.
Most commonly, statistical models of faces are built using component analysis
in order to leverage the highly correlated nature of different faces. Although
the most common form of component analysis,
Principal Component Analysis (PCA)~\cite{pearson1901lines,hotelling1933analysis},
is linear, the relationships between data are often nonlinear.
This has spurred a lot of interest in the development of
efficient and effective techniques for computing nonlinear dimensionality
reduction~\cite{yang2005kpca,goudelis2007class,scholkopf1998nonlinear}. These
nonlinear dimensionality reduction methods are interesting as they provide
a method for computing distances, or dissimilarity, between data that
lie in spaces that are not necessarily Euclidean. For example, if we wish to
perform subspace analysis on normals, we must consider the properties of
directional statistics such as surface normals.
A distribution of unit normals define a set of points that lie upon the
surface of the unit sphere. This implies that surface normals can be
parametrised as coordinates on the surface of the unit 2-sphere and thus
the computation of distances between normals is a non-trivial task. The
computation of distances between data elements is a key requirement for
component analysis techniques. For example, PCA can be derived in a number of
ways, one of which is expressed as the minimisation of the orthogonal
reconstruction error of a given set of data points. More precisely, given
a mean-centred data matrix, $\bb{X} \in \R^{p \times q}$, PCA can be expressed
as
%%%%%%%%%%%%%%%%%%%
\begin{equation*}
\begin{alignedat}{2}
	&\argmin_{\bb{W}} \quad &&\lVert \bb{X} - \bb{W} \bb{W}^T \bb{X} \rVert^2 \\
	&\quad \, \st      &&\bb{W} \bb{W}^T = \bb{I}_p
\end{alignedat}
\end{equation*}
%%%%%%%%%%%%%%%%%%%
where $\bb{W} \in \R^{p \times k}$ where $k \ll q$ and
$\bb{I} \in \R^{p \times p}$ is an identity matrix. This clearly demonstrates
that PCA minimizes the orthogonal Euclidean reconstruction error. In the PCA
reconstruction objective above, a mean-centred data matrix was assumed. This is
important as PCA may also be derived as a rotation around the mean that
maximises the variance of the data. Thus, the ability to accurately compute the
mean is an important requirement for correctly recovering principal components.
For normals, the computation of a correct mean
is complicated by the constraint that the mean normal must also lie on the
manifold of the unit 2-sphere. Unfortunately, a linear combination of unit
vectors is not guaranteed to yield a unit vector. Given the embedding of normals
in the 2-sphere manifold, the correct metric for normals is not the Euclidean
distance. This is illustrated in
\cref{fig:singl_img_norm_normal_distance}, which clearly shows that the Euclidean
distance represents an underestimate of the true distance between two unit
vectors lying on the surface of a sphere.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\begin{minipage}[t]{.45\textwidth}
	    \centering
		\includegraphics[width=0.9\textwidth]{statistical_normals/images/normal_distance}
		\captionof{figure}{The two arrow-header vectors represents normals
		                   embedded on the surface of the unit sphere. The red
		                   line is the Euclidean distance, which underestimates
		                   the true distance. The green arc is the metrically
		                   correct geodesic distance.}
\label{fig:singl_img_norm_normal_distance}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[t]{.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{statistical_normals/images/united_nations_logo}
		\captionof{figure}{The flag of the United Nations (UN) depicts
		                   an Azimuthal Equidistant projection centred on the
		                   North pole. Image of the UN flag is used under the
		                   terms of the public domain~\cite{un_flag}.}
\label{fig:singl_img_united_nations}
	\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Thus, any component analysis method that is to be performed on directional data
must be able to account for non-Euclidean distances and the manifold constraints
on the data mean. One method of computing component analysis on data that is
not related linearly and thus where Euclidean distances do not hold is
Kernel Principal Component Analysis (KPCA)~\cite{scholkopf1998nonlinear}.
KPCA is a non-linear generalisation of PCA that allows the computation
of component analysis within arbitrary dimensional Hilbert spaces,
including the subspace of normals. A Hilbert space is a generalisation
of a Euclidean space to arbitrary dimensions. More specifically, a Hilbert
space is a vector space possessing the structure of an inner
product. Thus, by providing a kernel function that defines
an inner product within a Hilbert space, we can perform component analysis in
spaces where PCA would normally be infeasible.

In this \namecref{ch:singl_imag}, we show the power of using KPCA to perform
component analysis of normals. The difference between the proposed framework and
previous works is that, instead of using off-the-shelf kernels such as Radial
Basis Functions (RBFs) or polynomial kernels, we are interested only in kernels
tailored to directional data. By incorporating the constraints of directional
data directly into our analysis we derive two novel kernels for performing
statistical analysis directly on normals. It is important to note that this is
not the first work to consider the construction of statistical models of surface
normals. \citet{smith2006recovering,smith2008facial} were the
first to note the complexities of performing component analysis on normals. To
solve this problem, \citet{smith2006recovering,smith2008facial}
proposed to borrow from the cartographic
community in order to define projection operators that map points on the surface
of a sphere to a tangent plane that preserves distances. This projection, called
the Azimuthal Equidistant Projection (AEP)~\cite{snyder1987map} is demonstrated
most commonly by the flag of the United Nations as depicted in
\cref{fig:singl_img_united_nations}.
\citet{smith2008facial} also investigated the use of
Principal Geodesic Analysis (PGA)~\cite{fletcher2004principal,smith2008facial}
for building statistical models of normals. In particular, PGA provides a
principled way to correctly compute the mean of a Riemannian manifold. However,
although the observation that computing distances between normals is non-trivial
is correct, this does not actually prevent component analysis being computed
directly on normals (\ie~without applying any transformation). By formulating
the component analysis in terms of a kernel, it becomes obvious that component
analysis \textit{can be performed directly on normals by defining the kernel as
the Euclidean inner product or cosine kernel}. We generalise AEP and PGA as
kernels in our framework and provide novel kernels that allow for the
computation of component analysis directly on normals, without the need for
projection into a tangent space.

Another intriguing property of surface normals is that they encode a local
measurement of surface orientation. Naturally, this means that a field of
surface normals provide insight into the curvature of the surface. This encoding
of local curvature shares many similarities to the concept of gradients in the
image domain. In fact, as discussed in \cref{sec:bg_sfs}, surface normals may be
parametrised as a gradient field directly and assuming the gradient field to be
integrable, may be integrated to recover 3D shape. The parametrisation of
surface normals as gradient fields is particularly interesting given the work of
\citet{tzimiropoulos2012subspace,tzimiropoulos2011robust,%
tzimiropoulos2014active} who show that gradient orientations can be coupled with
the cosine function in order to perform robust comparisons between images. In
particular, \citet{tzimiropoulos2012subspace} show that using the
cosine of normalised gradients as a dissimilarity measure allows gradients
to be effectively used for both component
analysis~\cite{tzimiropoulos2012subspace,tzimiropoulos2014active} and alignment
purposes~\cite{tzimiropoulos2011robust,tzimiropoulos2014active,%
antonakos2015feature}. The main result of the work on the cosine of normalised
gradients is that gross outliers, such as occlusions, are effectively suppressed
by the cosine function assuming the outliers are drawn from particular
distributions. This is an important result as it implies that the effect
of gross outliers for dissimilarity measurement is greatly reduced
under the distribution of normalised image gradients combined with
the cosine function. Inspired by this, we also investigate how the cosine
function might be used in conjunction with surface normals in order to build
robust dissimilarity measures. Therefore, in a similar manner to
\citet{tzimiropoulos2012subspace,tzimiropoulos2011robust,tzimiropoulos2014active},
we investigate the distribution of surface normals when combined with the
cosine function and provide applications for both component analysis and
alignment. As mentioned, we show the application of Kernel PCA for surface
normals and provide two novel kernels which follow from the combination
of surface normals with the cosine function. We also investigate parametric
alignment within the Lucas-Kanade~\cite{lucas1981iterative} framework
by employing both the cosine dissimilarity directly and with a statistical
model of surface normals. Following recent works on robust feature spaces for
alignment~\cite{antonakos2015feature}
we show that performing well known alignment algorithms such as
Lucas-Kanade~\cite{lucas1981iterative} on image descriptors (or features) can
provide significant improvements in alignment. Given a statistical model
of normals, we further investigate the properties of normals for rigid and
deformable alignment within the Lucas-Kanade framework. Specifically, we
investigate two separate modalities of data, \textbf{2.5D data} as is
commonly supplied by depth cameras and \textbf{3D data} as is more common
in the medical imaging community. For 2.5D data, we propose to use normals both
as a feature for Lucas-Kanade~\cite{lucas1981iterative} fitting,
as in the work of \citet{antonakos2015feature}, and as a statistical
appearance model for Active Appearance Model~\cite{cootes2001active} alignment.
For 3D data (or volumetric), we provide two novel
inverse compositional alignment~\cite{baker2004lucas} algorithms for robustly
aligning data that contains gross outliers such as occlusions.

The rest of this chapter continues as follows:
\cref{sec:singl_img_cosine_orientations} describes the various
parametrisations of surface normals and discusses how they may be combined with
the cosine function. Preliminary examples are provided for multiple
data sources that experimentally verify that the cosine of surface normals
can provide a dissimilarity that is robust to gross outliers. Furthermore,
the use of surface normals for component analysis is discussed by proposing a
Kernel PCA framework for normals. It also embeds the existing AEP and PGA
operators as kernels.
\cref{sec:singl_img_gsfs} demonstrates how the KPCA framework augments
the geometric SfS method of \citet{worthington1999new} in a similar manner to
\citet{smith2006recovering}.
\cref{sec:singl_imag_lk} demonstrates how surface
normals can be utilized for alignment within Lucas-Kanade style
algorithms~\cite{lucas1981iterative}. Two modalities of data are considered,
\cref{subsec:singl_img_lk_2d} demonstrates how a statistical model of normals
can be utilized to align 2.5D depth data by building an appearance model of
surface normals for use in an Active Appearance Model~\cite{cootes2001active}.
Finally, \cref{subsec:singl_img_lk_3d} demonstrates the use of normals for the
formation of a statistically robust rigid Lucas-Kanade~\cite{lucas1981iterative}
alignment method for 3D (volumetric) data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
% List of mathematical commands for this chapter
\input{statistical_normals/math_commands}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{statistical_normals/cosine_normalised_gradients}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{statistical_normals/component_analysis/base}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{statistical_normals/geometric_sfs/base}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{statistical_normals/lk/base}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\stopcontents[chapters]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
